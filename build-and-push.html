---
layout: default
title: Build And Push - Voldemort
---


<h1>Build and Push Jobs for Voldemort Read Only Stores</h1>

<h2>Introduction </h2>

<p>We have been using the Build and Push Job at Linkedin to create Voldemort Read-Only stores from data present in sequence files/ Avro Container files on HDFS.
  <br/>The Voldemort build and push job uses the fault tolerance and parallelism of Hadoop and builds individual Voldemort node/partition-level data stores, which are transferred to Voldemort for serving.
  <br/>A Hadoop job reads data from a source in HDFS, repartitions it on a per-node basis, and ﬁnally writes the data to individual Read-only storage engine [1].
  <br/>
  <br/>The VoldemortBuildAndPushJob will behave in the following way:
  <ol>
    <li>Build an XML storeDef for your data (based off of the key and value metadata in your JsonSequenceFile/Avro on HDFS).</li>
    <li>Connect to push.cluster</li>
    <li>Get the storeDefs for all stores in the push.cluster</li>
    <li>Look through the storeDefs for a store with the same name as push.store.name. If one is found, validate that the storeDef in the cluster matches the storeDef for your data. If it doesn’t, fail. If no storeDef exists on the cluster that matches push.store.name,
        then add your storeDef to the cluster.</li>
    <li>Build the Voldemort store in Hadoop</li>
    <li>Push the Voldemort store to push.cluster</li>
  </ol>
</p>

<h4> Azkaban Job </h4>

<p>The Build and Push Job is an Azkaban job. Azkaban is a workflow scheduler used at LinkedIn [2]. You provide a job file to Azkaban with a set of properties and the jars and Azkaban executes this job.</p>
<p>You can <a href="tarballs/hhw-new-version.tar.gz">download</a> the tarball. Then Untar it</p>

<div style="background: #202020; overflow:auto;width:auto;color:white;background:black;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
  <pre style="margin: 0; line-height: 125%"><span style="color: #d0d0d0">tar -xvf build-and-push.tar.gz</span></pre>
</div>
    
<h5>Structure </h5>

<div style="background: #f8f8f8; overflow:auto;width:auto;color:black;background:white;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
	<pre style="margin: 0; line-height: 125%">/conf
All Azkaban .job and .properties files must go in this directory.
/lib
All .jar files should go into this directory.
/src
All code should go into this directory. This includes Java, as well as scripting languages and shell scripts.
/test
Any testing code should be placed in this folder.</pre>
</div>
</p>
<p>PS: In case you are running Voldemort Server locally before you start the Voldemort server ensure that the server.properties file has the following entry</p>
  <div style="background: #f8f8f8; overflow:auto;width:auto;color:black;background:white;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
  	<pre style="margin: 0; line-height: 125%">file.fetcher.class=voldemort.store.readonly.fetcher.HdfsFetcher</pre>
  </div>
<p>This instructs the server to use that class while fetching files from Hadoop during the push phase</p>

<h4>Pushing JSON Data - Job File</h4>

<!-- HTML generated using hilite.me -->
<div style="background: #f8f8f8; overflow:auto;width:auto;color:black;background:white;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
	<pre style="margin: 0; line-height: 125%">type=java
job.class=voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob
hadoop.job.ugi=anagpal,hadoop
build.input.path=/tmp/new_op
build.output.dir=/tmp/build_output/
push.store.name=anagpal-test-old
push.cluster=tcp://localhost:6666
push.store.description="test store"
push.store.owners=myemail@myworkplace.com
build.replication.factor=1</pre>
</div>


<h4>Pushing AVRO Data - Job File</h4>

<div style="background: #f8f8f8; overflow:auto;width:auto;color:black;background:white;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
	<pre style="margin: 0; line-height: 125%">type=java
job.class=voldemort.store.readonly.mr.azkaban.VoldemortBuildAndPushJob
build.input.path=/user/anagpal/avro-data
build.output.dir=/tmp
push.cluster=tcp://localhost:6666
azkaban.should.proxy=true
user.to.proxy= anagpal
build.replication.factor= 1
build.type.avro=true
build.output.dir=/tmp/
avro.key.field=memberId
avro.value.field=localizedFirstNames
push.store.name=test-avro-store
push.store.description="Testing avro build and push"
push.store.owners= myemail@myworkplace.com
build.input.path=/user/anagpal/avro-data
build.output.dir=/tmp</pre>
</div>

<p>Notice the following properties:</p>
<ol>
  <li>build.type.avro=true This specifies that input data is Avro.</li>
  <li>avro.key.field=memberId This specifies the field to be used as the key</li>
  <li>avro.value.field=localizedFirstNames This specifies the field to be used as the value</li>
</ol>

<h4>Running the Job</h4>

<div style="background: #202020; overflow:auto;width:auto;color:white;background:black;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
    <pre style="margin: 0; line-height: 125%"><span style="color: #d0d0d0">ant</span>
</pre>
</div>
<p>Then after compiling on your hadoop gateway/local machine copy the directory and execute the following command:</p>
<div style="background: #202020; overflow:auto;width:auto;color:white;background:black;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
  <pre style="margin: 0; line-height: 125%"><span style="color: #d0d0d0">./run-job dist/build-and-push-1.00-all.jar -j dist/package/ -c dist/package/ --ignore-deps &lt;job name&gt;</span></pre>
</div>
<p>You need to change the input/output paths along with the ugi name,store name and server location. You can then query the voldemort server to see the new store entries</p>

<h4>File Format</h4>

<p>We use a custom data and index format for the Read-Only store.</p>
<p>Ony every Node you will find a node directory containing one or mutliple data and index files with the following naming convention:</p>
<!-- HTML generated using hilite.me -->
<div style="background: #f8f8f8; overflow:auto;width:auto;color:black;background:white;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;">
  <pre style="margin: 0; line-height: 125%">partition_id.replica_id.chunkset_id</pre>
</div>

<h4>Troubleshoot</h4>
    
<h5>Chunk size issue: </h5>
<dl>
  <dt>Symptom (possible): </dt>
  <dd>Caused by: java.io.IOException: Job failed! :</dd>
  <dt>Cause: </dt>
  <dd>Check the number of mappers and / or reducers (limit = 10000). If they're over the limit, use the num.chunks parameter to reduce number of chunks and hence #mappers, reducer</dd>
</dl>


<h5> Size limit  Chunk overflow exception: chunk  </h5>
<dl>
  <dt>Cause: </dt>
  <dd>Each chunk data file is capped at 2 Gb and hence you may want to increase the num.chunks to break it down into multiple chunks</dd>
</dl>

<h4>References</h4>

<ol>
    <li><a href="http://static.usenix.org/events/fast12/tech/full_papers/Sumbaly.pdf">Serving Large-scale Batch Computed Data with Project Voldemort</a></li>
    <li><a href="http://data.linkedin.com/opensource/azkaban">Azkaban</a></li>
</ol>
