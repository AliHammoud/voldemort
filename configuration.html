---
layout: default
title: Configuration - Voldemort
---

<h2>Configuration</h2>

<p>There are three configuration files that control server operation:</p>
<p>
<ul>
<li>
<i>cluster.xml</i> &ndash; This holds the information about all the nodes (i.e. servers) in the cluster, what hostname they are at, the ports they use, etc. It is exactly the same for all voldemort nodes. It does not hold tuning parameters or data directories for those nodes, since that is not information public to the cluster but is specific to that particular nodes configuration.
</li>
<li>
  <i>stores.xml</i> &ndash; This holds the information about all the stores (i.e. tables) in the cluster. This includes information about the required number of successful reads to maintain consistency, the required number of writes, as well as how keys and values are serialized into bytes. It is the same on all nodes in the cluster.</li>
<li><i>server.properties</i> &ndash; This contains the tuning parameters that control a particular node. This includes the id of the local node so it knows which entry in cluster.xml corresponds to itself, also the threadpool size, as well as any configuration needed for the local persistence engine such as BDB or mysql. This file is different on each node.
</li>
</ul>
</p>

<p>Finally there is an environment variable, VOLDEMORT_HOME, that controls the directory in which data and configuration reside. You can see an example of how the configuration is layed out in the config/ subdirectory of the project. This includes sample configurations that you can modify with your own specifics.</p>

<h3>Cluster configuration</h3>

<p>Here is an example cluster.xml for a 2-node cluster with 8 data partitions. We also have optional 'zone' fields which allow you to map nodes to certain logical clusters ( datacenter, rack, etc ) called zones:

<pre> 
  &lt;cluster&gt;
    &lt!-- The name is just to help users identify this cluster from the gui --&gt
    &lt;name&gt;mycluster&lt;/name&gt;
    &lt;zone&gt;
      &lt;zone-id&gt;0&lt;/zone-id&gt;
      &lt;proximity-list&gt;1&lt;/proximity-list&gt;
    &lt;zone&gt;
    &lt;zone&gt;
      &lt;zone-id&gt;1&lt;/zone-id&gt;
      &lt;proximity-list&gt;0&lt;/proximity-list&gt;
    &lt;zone&gt;
    &lt;server&gt;
      &lt!-- The node id is a unique, sequential id beginning with 0 that identifies each server in the cluster--&gt
      &lt;id&gt;0&lt;/id&gt;
      &lt;host&gt;vldmt1.prod.linkedin.com&lt;/host&gt;
      &lt;http-port&gt;8081&lt;/http-port&gt;
      &lt;socket-port&gt;6666&lt;/socket-port&gt;
      &lt;admin-port&gt;6667&lt;/admin-port&gt;
      &lt;!-- A list of data partitions assigned to this server --&gt;
      &lt;partitions&gt;0,1,2,3&lt;/partitions&gt;
      &lt;zone-id&gt;0&lt;/zone-id&gt;
    &lt;/server&gt;
    &lt;server&gt;
      &lt;id&gt;1&lt;/id&gt;
      &lt;host&gt;vldmt2.prod.linkedin.com&lt;/host&gt;
      &lt;http-port&gt;8081&lt;/http-port&gt;
      &lt;socket-port&gt;6666&lt;/socket-port&gt;
      &lt;admin-port&gt;6667&lt;/admin-port&gt;
      &lt;partitions&gt;4,5,6,7&lt;/partitions&gt;
      &lt;zone-id&gt;1&lt;/zone-id&gt;
    &lt;/server&gt;
  &lt;/cluster&gt;
</pre>

<p>
One thing that is important to understand is that partitions are not static partitions of servers, but rather they are a mechanism for partitioning the key space in such a way that each key is statically mapped to a particular data
partition. What this means is that a particular cluster may support many stores each with different replication factors&mdash;the replication factor is not hardcoded in the cluster design. This is important, since some data is more important than other data, and the correct trade-off between performance and consistency for one store may be different from another store. 
</p>
<p>
Another important point to remember is that the number of data partitions cannot be changed. We do support an online redistribution ( rebalancing ) of partitions. In other words inclusion of new nodes results in moving ownership of partitions, but the total number of partitions will always remain the same, as will the mapping of key to partition. This means it is important to give a good number of partitions to start with. The script <a href="https://github.com/voldemort/voldemort/blob/master/bin/generate_cluster_xml.py">here</a> will generate this part of the config for you.
</p>

<p>
Note that the configuration is currently simple files so it is important that the data in cluster.xml and stores.xml be exactly the same on each server, and that the node ids and partitions not be changed, since that can mean that clients will think their data should be on node <i>X</i> when really it was stored on node <i>Y</i>. This limitation will be removed as the configuration is moved into voldemort itself.
</p>

<h3>Store configuration</h3>

<p>Here is an examples stores.xml for a store named test, that requires only a single read and write and uses bdb for persistence:</p>

<pre>
  &lt;stores&gt;
      &lt;store&gt;
          &lt;name&gt;test&lt;/name&gt;
          &lt;replication-factor&gt;2&lt;/replication-factor&gt;
          &lt;preferred-reads&gt;2&lt;/preferred-reads&gt;
          &lt;required-reads&gt;1&lt;/required-reads&gt;
          &lt;preferred-writes&gt;2&lt;/preferred-writes&gt;
          &lt;required-writes&gt;1&lt;/required-writes&gt;
          &lt;persistence&gt;bdb&lt;/persistence&gt;
          &lt;routing&gt;client&lt;/routing&gt;
          &lt;routing-strategy&gt;consistent-routing&lt;/routing-strategy&gt;
          &lt;key-serializer&gt;
              &lt;type&gt;string&lt;/type&gt;
              &lt;schema-info&gt;utf8&lt;/schema-info&gt;
          &lt;/key-serializer&gt;
          &lt;value-serializer&gt;
              &lt;type&gt;json&lt;/type&gt;
              &lt;schema-info version="1"&gt;[{"id":"int32", "name":"string"}]&lt;/schema-info&gt;
              &lt;compression&gt;
      &lt;type&gt;gzip&lt;type&gt;
        &lt;/compression&gt;
          &lt;/value-serializer&gt;
      &lt;/store&gt;
  &lt;/stores&gt;
</pre>
<p>
Each of these parameters deserves a quick discussion:
<ul>
  <li><i>name</i>&mdash; The name of the store. This is the string by which clients will be able to connect and operate on this store. It is equivalent to the table name in sql.</li>
  <li><i>replication-factor</i>&mdash; This is the total number of times the data is stored. Each put or delete operation must eventually hit this many nodes. A replication factor of <i>n</i> means it can be possible to tolerate up to <i>n</i> - 1 node failures without data loss.</li>
  <li><i>preferred-reads</i> (optional)&mdash;The number of successful reads the client will attempt to do before returning a value to the application. This defaults to be equal to required reads</li>
  <li><i>required-reads</i>&mdash;The least number of reads that can succeed without throwing an exception. Consider a case where the replication factor is 5, preferred reads is 4, and required reads is 2. If 3 of the 5 nodes are operational then the client may try all the nodes to try to reach the preferred 4 reads, but since only 3 are responsive it will allow the read to complete. Had only 1 been responsive it would have thrown an exception, since that was lower than the consistency guarantee requested for this table (and that could mean returning stale data).</li>
  <li><i>preferred-writes</i>(optional)&mdash;The number of successful writes the client attempts to block for before returning success. Defaults to required-writes</li>
  <li><i>required-writes</i>&mdash; The least number of writes that can succeed without the client getting back an exception.</li>
  <li><i>persistence</i>&mdash; The persistence backend used by the store. Currently this could be one of <i>bdb</i>, <i>mysql</i>, <i>memory</i>, <i>readonly</i>, and <i>cache</i>. The difference between <i>cache</i> and <i>memory</i> is that <i>memory</i> will throw and OutOfMemory exception if it grows larger than the JVM heap whereas <i>cache</i> will discard data.</li>
  <li><i>routing</i>&mdash; Determines the routing policy. We support both <i>client</i> ( Client side routing ) and <i>server</i> ( Server side routing ).</li>
  <li><i>routing-strategy</i>&mdash; Determines how we store the replicas. Currently we support three routing-strategies - <i>consistent-routing</i> (default), <i>zone-routing</i> and <i>all-routing</i>.</li>
  <li><i>key-serializer</i>&mdash; The serialization type used for reading and writing <i>keys</i>. The type can be <i>json</i>, <i>java-serialization</i>, <i>string</i>, <i>protobuff</i>, <i>thrift</i>, or <i>identity</i> (meaning raw bytes). The schema-info gives information to the serializer about how to perform the mapping (e.g. the JSON schema described in <a href="design.html">here</a>).
  </li>
  <li><i>value-serializer</i>&mdash; The serialization type used for reading and writing <i>values</i>. The supported types are the same as for keys. In the above example we also highlight the subelement 'compression' which currently supports 'gzip' and 'lzf' compression. The subelements are same as for the key-serializer, except that the the value serializer can have multiple schema-infos with different versions. The highest version is the one used for writing data, but data is always read with the version it was written with. This allows for gradual schema evolution. Versioning is only supported by the JSON serializer as other serialization formats have their own versioning systems.
    Here are some example serializers:
      <pre>
    &lt;!-- A serializer that serializes plain strings in UTF8 encoding --&gt;
    &lt;value-serializer&gt;
        &lt;type&gt;string&lt;/type&gt;
        &lt;schema-info&gt;utf8&lt;/schema-info&gt;
    &lt;/value-serializer&gt;

    &lt;!-- A serializer that serializes binary-format JSON data with the given schema. 
            Each value is a List&lt;Map&lt;String, ?&gt;&gt; where the keys "id" and "name" and the values 
            are a 32-bit integer id and a string name. --&gt;
    &lt;value-serializer&gt;
        &lt;type&gt;json&lt;/type&gt;
        &lt;schema-info&gt;[{"id":"int32", "name":"string"}]&lt;/schema-info&gt;
    &lt;/value-serializer&gt;
    
    &lt;!-- A serializer that serializes protocol buffer objects of the given class. --&gt;
    &lt;value-serializer&gt;
        &lt;type&gt;protobuff&lt;/type&gt;
        &lt;schema-info&gt;java=com.something.YourProtoBuffClassName&lt;/schema-info&gt;
    &lt;/value-serializer&gt;
    
    &lt;!-- A serializer that serializes thrift generated objects using one of the 
            following protocols - 'binary', 'json' or 'simple-json'.
      Current support for Java clients only. --&gt;
    &lt;value-serializer&gt;
        &lt;type&gt;thrift&lt;/type&gt;
        &lt;schema-info&gt;java=com.something.YourThriftClassName,protocol=binary&lt;/schema-info&gt;
    &lt;/value-serializer&gt;
      
    &lt;!-- Avro serialization - either 'avro-generic', 'avro-specific' or 'avro-reflective' --&gt;
    &lt;value-serializer&gt;
        &lt;type&gt;avro-generic&lt;/type&gt;
        &lt;schema-info&gt;{"name": "Kind", "type": "enum", "symbols": ["FOO","BAR"]}&lt;/schema-info&gt;
    &lt;/value-serializer&gt;
      </pre>
    </li>
        </li>
        <li><i>retention-days</i> (optional)&mdash; This optional parameter allows you to set a retention property to your data. Then every day, at a specified time on the servers, a scheduled job will be run to delete all data having timestamp > retention-days. This is useful to keep your data trimmed. </li>
        <li><i>retention-scan-throttle-rate</i> (optional)&mdash; If <i>retention-days</i> is specified this is the rate at which we'll scan the tuples to delete data.</li>
</ul>
</p>
<p>
If you intend to use the <i>zone-routing</i> strategy we need to extend the store definition to tell it how to replicate w.r.t. zones. Here is an example of a store definition with 'zone-routing' enabled.
<pre>
  &lt;stores&gt;
      &lt;store&gt;
          &lt;name&gt;test&lt;/name&gt;
          ...
                &lt;routing-strategy&gt;zone-routing&lt;/routing-strategy&gt;
                &lt;!-- This number should be total of individual zone-replication-factor's --&gt;
                &lt;replication-factor&gt;2&lt;/replication-factor&gt;
          &lt;zone-replication-factor&gt;
              &lt;replication-factor zone-id="0"&gt;1&lt;/replication-factor&gt;
              &lt;replication-factor zone-id="1"&gt;1&lt;/replication-factor&gt;
          &lt;/zone-replication-factor&gt;
          &lt;zone-count-reads&gt;0&lt;/zone-count-reads&gt;
          &lt;zone-count-writes&gt;0&lt;/zone-count-writes&gt;
          &lt;hinted-handoff-strategy&gt;proximity-handoff&lt;/hinted-handoff-strategy&gt;
          ... 
           &lt;/store&gt;
  &lt;/stores&gt;
</pre>
 The important change here is the introduction of <i>zone-replication-factor</i> which should contain a replication factor that you would want in every zone. Other parameters :
<ul>
  <li><i>zone-count-*</i>&mdash; The number of zones we want to block for during reads / writes before we return the request. The number <i>0</i> means we'll block for atleast one request from the <i>local</i> zone only. The number <i>1</i> means we'll block for atleast one request from one other zone.</li> 
        <li><i>hinted-handoff-strategy</i> (optional) &mdash; Another consistency mechanism which we've added recently is <a href="https://github.com/voldemort/voldemort/wiki/Hinted-Handoff">Hinted handoff</a>. We can turn on this feature on a per store basis. This parameter defines the strategy we would use to decide which live nodes to write our "hint" to. The various options are <i>any-handoff</i>, <i>consistent-handoff</i> and <i>proximity-handoff</i>. 
</ul>
</p>

<h3>Server configuration</h3>

<p>We store per-node based configuration in the server.properties file. Most of the properties have sane defaults ( hopefully ). The bare minimal file should have the following property.</p> 

<pre>
  # The ID of *this* particular cluster node (different for each node in cluster)
  node.id=0
</pre>


You can find the list of server side parameters <a href="http://www.project-voldemort.com/voldemort/javadoc/all/voldemort/server/VoldemortConfig.html"> here </a>

Here is a list of all the configuration options supported:

<iframe height="500px" width="1000px" src="http://www.project-voldemort.com/voldemort/javadoc/all/voldemort/server/VoldemortConfig.html"></iframe> 



<!--
<table class="data-table">
<tr>
  <th>name</th>
  <th>default</th>
  <th>description</th>
</tr>
<tr>
  <td>node.id</td>
        <td>none</td>
        <td>The unique, sequential identifier for this server in the cluster (starts with 0)</td>
</tr>
<tr>
  <td>voldemort.home</td>
  <td>none</td>
  <td>The base directory for voldemort. Can also be specified via the environment variable VOLDEMORT_HOME or via a command line option.</td>
</tr>
<tr>
  <td>data.directory</td>
  <td>${voldemort.home}/data</td>
  <td>The directory where voldemort data is stored</td>
</tr>
<tr>
  <td>metadata.directory</td>
  <td>${voldemort.home}/config</td>
  <td>The directory where voldemort configuration is stored</td>
</tr>
<tr>
  <th colspan="3">BDB stores configuration</th>
</tr>
<tr>
  <td>enable.bdb.engine</td>
  <td>true</td>
  <td>Should the BDB engine be enabled?</td>
</tr>
<tr>
  <td>bdb.cache.size</td>
  <td>200MB</td>
  <td>The BDB cache that is shared by all BDB tables. Bigger is better.</td>
</tr>
<tr>
  <td>bdb.write.transactions</td>
  <td>false</td>
  <td>Should transactions be immediately written to disk?</td>
</tr>
<tr>
  <td>bdb.flush.transactions</td>
  <td>false</td>
  <td>When the transaction has been written to disk should we force the disk to flush the OS cache. This is a fairly expensive operation.</td>
</tr>
<tr>
  <td>bdb.data.directory</td>
  <td>${data.directory}/bdb</td>
  <td>The directory where the BDB environment is located</td>
</tr>
<tr>
  <td>bdb.max.logfile.size</td>
  <td>1GB</td>
  <td>The size of an individual log file</td>
</tr>
<tr>
  <td>bdb.btree.fanout</td>
  <td>512</td>
  <td>The fanout size for the btree. Bigger fanout more effienciently supports larger btrees.</td>
</tr>
<tr>
  <td>bdb.checkpoint.interval.bytes</td>
  <td>20 * 1024 * 1024</td>
  <td>How often (in bytes) should we checkpoint the transaction log? Checkpoints make startup and shutdown faster.</td>
</tr>
<tr>
  <td>bdb.checkpoint.interval.ms</td>
  <td>30000</td>
  <td>How often in ms should we checkpoint the transaction log</td>
</tr>
<tr>
  <td>bdb.one.env.per.store</td>
  <td>false</td>
  <td>Use one BDB environment for every store</td>
</tr>
<tr>
  <td>bdb.cleaner.threads</td>
  <td>1</td>
  <td>Number of BDB cleaner threads</td>
</tr>
<tr>
      <th colspan="3">MySQL stores configuration</th>
</tr>
<tr>
  <td>enable.mysql.engine</td>
  <td>false</td>
  <td>Should we enabled the mysql storage engine? Doing so will create a connection pool that will be used for the mysql instance</td>
</tr>
<tr>
  <td>mysql.user</td>
  <td>root</td>
  <td>The mysql username to user</td>
</tr>
<tr>
  <td>mysql.password</td>
  <td></td>
  <td>The mysql password to user</td>
</tr>
<tr>
  <td>mysql.host</td>
  <td>localhost</td>
  <td>The host of the mysql instance</td>
</tr>
<tr>
  <td>mysql.port</td>
  <td>3306</td>
  <td>The port of the mysql instance</td>
</tr>
<tr>
  <td>mysql.database</td>
  <td>voldemort</td>
  <td>The name of the mysql database</td>
</tr>
<tr>
      <th colspan="3">Read-only stores configuration</th>
</tr>
<tr>
  <td>enable.readonly.engine</td>
  <td>false</td>
  <td>Should we enable the readonly storage engine?</td>
</tr>
<tr>
  <td>readonly.backups</td>
  <td>1</td>
  <td>The number of backup copies of the data to keep around for rollback.</td>
</tr>
<tr>
  <td>readonly.search.strategy</td>
  <td>BinarySearchStrategy</td>
  <td>Class name of search strategy to use while finding key. We support BinarySearchStrategy and InterpolationSearchStrategy</td>
</tr>
<tr>
  <td>readonly.data.directory</td>
  <td>${data.directory}/read-only</td>
  <td>The directory in which to store readonly data files.</td>
</tr>
<tr>
  <td>readonly.delete.backup.ms</td>
  <td>0</td>
  <td>Millisecond we wait for before deleting old data. Useful to decreasing IO during swap.</td>
</tr>
<tr>
  <th colspan="3">Slop store configuration</th>
</tr>
<tr>
  <td>slop.enable</td>
  <td>true</td>
  <td>Do we want to initialize a storage engine for slops + have the job enabled?</td>
</tr>
<tr>
  <td>slop.store.engine</td>
  <td>bdb</td>
  <td>What storage engine should we use for storing misdelivered messages that need to be rerouted?</td>
</tr>
<tr>
  <td>slop.pusher.enable</td>
  <td>true</td>
  <td>Enable the slop pusher job which pushes every 'slop.frequency.ms' ms ( Prerequisite - slop.enable=true )</td>
</tr>
<tr>
  <td>slop.read.byte.per.sec</td>
  <td>10 * 1000 * 1000</td>
  <td>Slop max read throughput </td>
</tr>
<tr>
  <td>slop.write.byte.per.sec</td>
  <td>10 * 1000 * 1000</td>
  <td>Slop max write throughput </td>
</tr>
<tr>
  <td>pusher.type</td>
  <td>StreamingSlopPusherJob</td>
  <td>Job type to use for pushing out the slops</td>
</tr>
<tr>
  <td>slop.frequency.ms</td>
  <td>5 * 60 * 1000</td>
  <td>Frequency at which we'll try to push out the slops </td>
</tr>
<tr>
  <th colspan="3">Rebalancing configuration</th>
</tr>
<tr>
  <td>enable.rebalancing</td>
  <td>true</td>
  <td>Enable rebalance service?</td>
</tr>
<tr>
  <td>max.rebalancing.attempts</td>
  <td>3</td>
  <td>Number of attempts the server side rebalancer makes to fetch data</td>
</tr>
<tr>
  <td>rebalancing.timeout.seconds</td>
  <td>10 * 24 * 60 * 60</td>
  <td>Time we give for the server side rebalancing to finish copying data</td>
</tr>
<tr>
  <td>max.parallel.stores.rebalancing</td>
  <td>3</td>
  <td>Stores to rebalancing in parallel</td>
</tr>
<tr>
  <td>rebalancing.optimization</td>
  <td>true</td>
  <td>Should we run our rebalancing optimization for non-partition aware stores?</td>
</tr>
<tr>
  <th colspan="3">Retention configuration</th>
</tr>
<tr>
  <td>retention.cleanup.first.start.hour</td>
  <td>0</td>
  <td>Hour when we want to start the first retention cleanup job</td>
</tr>
<tr>
  <td>retention.cleanup.period.hours</td>
  <td>24</td>
  <td>Run the retention clean up job every n hours</td>
</tr>
<tr>
  <th colspan="3">Gossip configuration</th>
<tr>
<tr>
  <td>enable.gossip</td>
  <td>false</td>
  <td>Enable gossip to synchronize state</td>
</tr>
<tr>
  <td>gossip.interval.ms</td>
  <td>30*1000</td>
  <td>Enable gossup every n ms</td>
</tr>
<tr>
  <th colspan="3">Admin service</th>
</tr>
<tr>
  <td>admin.enable</td>
  <td>true</td>
  <td>Enable the Admin service?</td>
</tr>
<tr>
  <td>admin.max.threads</td>
  <td>20</td>
  <td>Max Number of threads used by Admin services. Used by BIO ( i.e. if enable.nio.connector = false )</td>
</tr>
<tr>
  <td>admin.core.threads</td>
  <td>max(1, ${admin.max.threads} / 2)</td>
  <td>The number of threads to keep alive by Admin service even when idle. Used by BIO ( i.e. if enable.nio.connector = false )</td>
</tr>
<tr>
  <td>nio.admin.connector.selectors</td>
  <td>max ( 8, number of processors )</td>
  <td>Number of selector threads for admin operations. Used by NIO ( i.e. if enable.nio.connector = true )</td>
</tr>
<tr>
  <th colspan="3">Core Voldemort server configuration</th>
<tr>
<tr>
  <td>enable.nio.connector</td>
  <td>true</td>
  <td>Enable NIO on server side</td>
</tr>
<tr>
  <td>nio.connector.selectors</td>
  <td>max ( 8, number of processors )</td>
  <td>Number of selector threads for normal operations. Used by NIO ( i.e. if enable.nio.connector = true )</td>
</tr>
<tr>
  <td>max.threads</td>
  <td>100</td>
  <td>The maximum number of threads the server can use ( Used by HTTP and BIO - enable.nio.connector = false -  service only )</td>
</tr>
<tr>
  <td>core.threads</td>
  <td>max(1, ${max.threads} / 2)</td>
  <td>The number of threads to keep alive even when idle ( Used by HTTP and BIO - enable.nio.connector = false -  service only )</td>
</tr>
<tr>
  <td>socket.timeout.ms</td>
  <td>4000</td>
  <td>The socket SO_TIMEOUT. Essentially the amount of time to block on a low-level network operation before throwing an error.</td>
</tr>
<tr>
  <td>routing.timeout.ms</td>
  <td>5000</td>
  <td>The total amount of time to wait for adequate responses from all nodes before throwing an error.</td>
</tr>
<tr>
  <td>stream.read.byte.per.sec</td>
  <td>10 * 1000 * 1000</td>
  <td>Max read throughput allowed when Admin service streams data</td>
</tr>
<tr>
  <td>stream.write.byte.per.sec</td>
  <td>10 * 1000 * 1000</td>
  <td>Max write throughput allowed when Admin service streams data</td>
</tr>
<tr>
  <td>http.enable</td>
  <td>true</td>
  <td>Enable the HTTP data server?</td>
</tr>
<tr>
  <td>socket.enable</td>
  <td>true</td>
  <td>Enable the socket data server?</td>
</tr>
<tr>
  <td>jmx.enable</td>
  <td>true</td>
  <td>Enable JMX monitoring?</td>
</tr>
<tr>
  <td>enable.verbose.logging</td>
  <td>true</td>
  <td>Log every operation on all stores.</td>
</tr>
<tr>
  <td>enable.stat.tracking</td>
  <td>true</td>
  <td>Track load statistics on the stores.</td>
</tr>
<tr>
  <td>scheduler.threads</td>
  <td>6</td>
  <td>Number of threads to use for scheduled jobs</td>
</tr>
</table>
-->


<h3>BDB Management</h3>

<p>The underlying key-value store is also important for configuration and operation management. If BDB is used then all configuration is done through the server.properties file. If MySQL is used then usual mysql administration must be done.</p>

<p>Oracle has <a href="http://download.oracle.com/docs/cd/E17277_02/html/GettingStartedGuide/backuprestore.html">a writeup</a> that gives a good overview of the operational side of BDB.</p>

<h3>Client configuration</h3>

<p>The above settings were all for the server. It is important to correctly configure the client as well:

List of all the client configuration parameters can be found <a href="http://www.project-voldemort.com/voldemort/javadoc/all/voldemort/client/ClientConfig.html">here</a>

<iframe src="http://www.project-voldemort.com/voldemort/javadoc/all/voldemort/client/ClientConfig.html" width="1000px" height="500px"></iframe>




<!--
<table class="data-table">
<tr>
  <th>name</th>
  <th>default</th>
  <th>description</th>
</tr>
<tr>
  <td>max_connections</td>
        <td>50</td>
        <td>Maximum number of connection allowed to each voldemort node</td>
</tr>
<tr>
  <td>max_threads</td>
  <td>5</td>
  <td>The maximum number of client threads ( Used by the client thread pool )</td>
</tr>
<tr>
  <td>max_queued_requests</td>
  <td>50</td>
  <td>The maximum number of queued node operations before client actions will be blocked ( Used by the client thread pool )</td>
</tr>
<tr>
  <td>thread_idle_ms</td>
  <td>100000</td>
  <td>The amount of time to keep an idle client thread alive ( Used by the client thread pool )</td>
</tr>
<tr>
  <td>connection_timeout_ms</td>
  <td>500</td>
  <td>Set the maximum allowable time to block waiting for a free connection</td>
</tr>
<tr>
  <td>socket_timeout_ms</td>
  <td>5000</td>
  <td>Maximum amount of time the socket will block waiting for network activity</td>
</tr>
<tr>
  <td>routing_timeout_ms</td>
  <td>15000</td>
  <td>Set the timeout for all blocking operations to complete on all nodes. The number of blocking operations can be configured using the preferred-reads and preferred-writes configuration for the store.</td>
</tr>
<tr>
  <td>selectors</td>
  <td>8</td>
  <td>Number of selectors used for multiplexing requests in our NIO client</td>
</tr>
<tr>
  <td>socket_buffer_size</td>
  <td>64 * 1024</td>
  <td>Set the size of the socket buffer (in bytes) to use for both socket reads and socket writes</td>
</tr>
<tr>
  <td>enable_jmx</td>
  <td>true</td>
  <td>Enable JMX monitoring</td>
</tr>
<tr>
  <td>enable_pipeline_routed_store</td>
  <td>true</td>
  <td>Use the new pipeline routed store for client side routing</td>
</tr>
<tr>
  <td>max_bootstrap_retries</td>
  <td>2</td>
  <td>Number of times we'll try to connect to bootstrap url</td>
</tr>
<tr>
  <td>bootstrap_urls</td>
  <td>Compulsory parameter</td>
  <td>Comma separated list of URLs to use as bootstrap servers</td>
</tr>
<tr>
  <td>serializer_factory_class</td>
  <td>Default serializer factory with support for avro, pb, java, etc.</td>
  <td>Custom serializer factory class name</td>
</tr>
<tr>
  <td>client_zone_id</td>
  <td>0</td>
  <td>Zone id where the client resides. Used to make smarter routing decision in case of 'zone-routing'</td>
</tr>
<tr>
        <th colspan="3">Failure detector configs</th>
</tr>
<tr>
  <td>failuredetector_implementation</td>
  <td>ThresholdFailureDetector</td>
  <td>Class name of the failure detector that the client will use. We support BannagePeriodFailureDetector and ThresholdFailureDetector</td>
</tr>
<tr>
  <td>failuredetector_bannage_period</td>
  <td>30000</td>
  <td>BannagePeriodFailureDetector : The number of milliseconds this node is considered as 'banned'</td>
</tr>
<tr>
  <td>failuredetector_threshold_countminimum</td>
  <td>30</td>
  <td>ThresholdFailureDetector : Minimum number of failures that must occur before the success ratio is checked against the threshold</td>
</tr>
<tr>
  <td>failuredetector_threshold_interval</td>
  <td>30000</td>
  <td>ThresholdFailureDetector : Millisecond interval for which the threshold is valid; it is 'reset' after this period is exceeded</td>
</tr>
<tr>
  <td>failuredetector_threshold</td>
  <td>95</td>
  <td>ThresholdFailureDector : The integer percentage representation of the threshold that must be met or exceeded</td>
</tr>
</table>
-->

<h3>Some additional suggestions</h3>

<h4>JVM Settings</h4>

At LinkedIn we maintain two sets of clusters, read-only and read-write. The read-write clusters are clusters using BDB stores and have totally different JVM characteristics from those using read-only stores. Here is what we use at LinkedIn for our read-write stores:

<pre>
  # Min, max, total JVM size 
  JVM_SIZE="-server -Xms32g -Xmx32g"

  # New Generation Sizes 
  JVM_SIZE_NEW="-XX:NewSize=2048m -XX:MaxNewSize=2048m"

  # Type of Garbage Collector to use
  JVM_GC_TYPE="-XX:+UseConcMarkSweepGC -XX:+UseParNewGC"

  # Tuning options for the above garbage collector
  JVM_GC_OPTS="-XX:CMSInitiatingOccupancyFraction=70 -XX:SurvivorRatio=2"

  # JVM GC activity logging settings
  JVM_GC_LOG="-XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -Xloggc:$LOG_DIR/gc.log"
</pre>

Note that you must use the concurrent mark and sweep gc or else the GC pauses from collecting such a large heap will cause unresponsive periods (it doesn't happen at first either, it creeps up and then eventually goes into a spiral of gc pause death). 

This is the setup on a 48GB RAM box with a BDB cache size of 20GB and 1 cleaner threads, on SSDs. 
You can find the entire configuration under config/prod_single_node_cluster. To bring a server up with these settings, use bin/voldemort-prod-server.sh

<p>
For the read-only clusters we use the same JVM GC settings, except the heap size is set to a smaller value.
</p>
<pre>
  # Min, max, total JVM size 
  JVM_SIZE="-server -Xms4096m -Xmx4096m"
</pre>

This is done because in the case of read-only stores we rely on the OS page cache and don't really want our JVM heap to take up space. 
